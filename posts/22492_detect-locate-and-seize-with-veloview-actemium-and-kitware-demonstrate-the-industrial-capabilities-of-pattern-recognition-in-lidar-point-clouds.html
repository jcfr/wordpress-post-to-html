<meta charset="utf-8">
<div class="entry-content">
 <p>
  <img alt="" class=" wp-image-22495 alignleft" height="152" sizes="(max-width: 283px) 100vw, 283px" src="http://blog.kitware.com/wp-content/uploads/2018/05/featuring_logo-300x161.png" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/featuring_logo-300x161.png 300w, https://blog.kitware.com/wp-content/uploads/2018/05/featuring_logo-220x118.png 220w, https://blog.kitware.com/wp-content/uploads/2018/05/featuring_logo-250x134.png 250w, https://blog.kitware.com/wp-content/uploads/2018/05/featuring_logo-355x190.png 355w, https://blog.kitware.com/wp-content/uploads/2018/05/featuring_logo-90x48.png 90w, https://blog.kitware.com/wp-content/uploads/2018/05/featuring_logo.png 450w" width="283"/>
 </p>
 <p style="text-align: left">
  Kitware and Actemium are pleased to present their fruitful collaboration using a Lidar to detect, locate and seize objects on a construction site for semi-automated ground drilling. Actemium’s overall mission is to automate the drilling process of pile foundations for buildings, in which multiple hollow tubes are used, and then filled with concrete.
 </p>
 <div class="wp-caption alignright" id="attachment_22497" style="width: 251px">
  <a href="http://blog.kitware.com/wp-content/uploads/2018/05/KukaRobot.png">
   <img alt="" class="wp-image-22497 " height="266" sizes="(max-width: 241px) 100vw, 241px" src="http://blog.kitware.com/wp-content/uploads/2018/05/KukaRobot-272x300.png" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/KukaRobot-272x300.png 272w, https://blog.kitware.com/wp-content/uploads/2018/05/KukaRobot-768x848.png 768w, https://blog.kitware.com/wp-content/uploads/2018/05/KukaRobot-927x1024.png 927w, https://blog.kitware.com/wp-content/uploads/2018/05/KukaRobot-220x243.png 220w, https://blog.kitware.com/wp-content/uploads/2018/05/KukaRobot-250x276.png 250w, https://blog.kitware.com/wp-content/uploads/2018/05/KukaRobot-355x392.png 355w, https://blog.kitware.com/wp-content/uploads/2018/05/KukaRobot-730x806.png 730w, https://blog.kitware.com/wp-content/uploads/2018/05/KukaRobot-90x99.png 90w" width="241"/>
  </a>
  <p class="wp-caption-text">
   Fig. 1: KUKA robot with the LiDAR sensor mounted on it
  </p>
 </div>
 <p>
 </p>
 <p>
 </p>
 <p>
  The collaboration aims to automate the drilling process by replacing the manual addition of drilling tubes with a semi-automatic one. A KUKA robot arm -driven by Kitware vision algorithms- has to put additional tubes into the driller head for it to continue drilling down. Actemium chose a Velodyne Lidar sensor as an input, and asked Kitware to make the pattern recognition algorithms based on VeloView.
 </p>
 <p>
 </p>
 <p>
 </p>
 <div class="wp-caption alignleft" id="attachment_22517" style="width: 210px">
  <a href="http://blog.kitware.com/wp-content/uploads/2018/05/foreuse.jpg">
   <img alt="" class="wp-image-22517 " height="267" sizes="(max-width: 200px) 100vw, 200px" src="http://blog.kitware.com/wp-content/uploads/2018/05/foreuse-225x300.jpg" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/foreuse-225x300.jpg 225w, https://blog.kitware.com/wp-content/uploads/2018/05/foreuse-768x1024.jpg 768w, https://blog.kitware.com/wp-content/uploads/2018/05/foreuse-220x293.jpg 220w, https://blog.kitware.com/wp-content/uploads/2018/05/foreuse-250x333.jpg 250w, https://blog.kitware.com/wp-content/uploads/2018/05/foreuse-355x473.jpg 355w, https://blog.kitware.com/wp-content/uploads/2018/05/foreuse-730x973.jpg 730w, https://blog.kitware.com/wp-content/uploads/2018/05/foreuse-90x120.jpg 90w" width="200"/>
  </a>
  <p class="wp-caption-text">
   Fig. 2: Real driller on a construction site
  </p>
 </div>
 <p>
  VeloView is a free open-source Paraview-based application developed by Kitware. It is designed to visualize and analyse raw data from the Velodyne LiDAR sensors as 3D point cloud. Thanks to its open-source nature, customer-specific versions are regularly developed, containing specific enhancements such as:
  <br/>
  –
  <b>
   Drivers
  </b>
  : IMU, GPS or any specific hardware
  <br/>
  –
  <b>
   Algorithms
  </b>
  : point cloud filtering and analysis, deep learning for object recognition (pedestrians, traffic signs, vehicles, …), semantic classification, SLAM for automotive or UAV, dense 3D reconstruction, …
  <br/>
  –
  <b>
   Custom graphical interface
  </b>
  to simplify customer processes, or to show the live detection of objects or pedestrians
 </p>
 <p>
 </p>
 <div class="wp-caption alignright" id="attachment_22499" style="width: 225px">
  <a href="http://blog.kitware.com/wp-content/uploads/2018/05/VeloView.jpg">
   <img alt="" class="wp-image-22499 " height="287" sizes="(max-width: 215px) 100vw, 215px" src="http://blog.kitware.com/wp-content/uploads/2018/05/VeloView-225x300.jpg" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/VeloView-225x300.jpg 225w, https://blog.kitware.com/wp-content/uploads/2018/05/VeloView-768x1024.jpg 768w, https://blog.kitware.com/wp-content/uploads/2018/05/VeloView-220x293.jpg 220w, https://blog.kitware.com/wp-content/uploads/2018/05/VeloView-250x333.jpg 250w, https://blog.kitware.com/wp-content/uploads/2018/05/VeloView-355x473.jpg 355w, https://blog.kitware.com/wp-content/uploads/2018/05/VeloView-730x973.jpg 730w, https://blog.kitware.com/wp-content/uploads/2018/05/VeloView-90x120.jpg 90w" width="215"/>
  </a>
  <p class="wp-caption-text">
   Fig. 3: VeloView on the embedded Beckhoff computer
  </p>
 </div>
 <p>
 </p>
 <p>
  Kitware’s task was to first scan the scene at the beginning of a new pile drilling, in order to detect the driller’s position and orientation. Secondly, for each tube addition, we need to locate the head of the previous tube. Finally, we send those information for the robot to move the new tube in position.
 </p>
 <p>
  A high density point cloud of the scene is acquired, by moving the LiDAR sensor to compensate for its sparse nature. Here a VLP-16 is used (2 degrees of vertical angular resolution between each of the 16 rotating laser-beams, and up to 120m range with 2 cm accuracy). Kitware programmed the KUKA arm relative motion computation and merged the acquired point clouds into the KUKA’s coordinate system.
 </p>
 <div class="wp-caption aligncenter" id="attachment_22610" style="width: 1034px">
  <a href="http://blog.kitware.com/wp-content/uploads/2018/05/ScanForeuseSceneParaview-1.png">
   <img alt="" class="wp-image-22610 size-large" height="482" sizes="(max-width: 1024px) 100vw, 1024px" src="http://blog.kitware.com/wp-content/uploads/2018/05/ScanForeuseSceneParaview-1-1024x482.png" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/ScanForeuseSceneParaview-1-1024x482.png 1024w, https://blog.kitware.com/wp-content/uploads/2018/05/ScanForeuseSceneParaview-1-300x141.png 300w, https://blog.kitware.com/wp-content/uploads/2018/05/ScanForeuseSceneParaview-1-768x362.png 768w, https://blog.kitware.com/wp-content/uploads/2018/05/ScanForeuseSceneParaview-1-220x104.png 220w, https://blog.kitware.com/wp-content/uploads/2018/05/ScanForeuseSceneParaview-1-250x118.png 250w, https://blog.kitware.com/wp-content/uploads/2018/05/ScanForeuseSceneParaview-1-355x167.png 355w, https://blog.kitware.com/wp-content/uploads/2018/05/ScanForeuseSceneParaview-1-730x344.png 730w, https://blog.kitware.com/wp-content/uploads/2018/05/ScanForeuseSceneParaview-1-90x42.png 90w" width="1024"/>
  </a>
  <p class="wp-caption-text">
   Fig. 4: KUKA robot arm scanning a mockup development scene (left) and the corresponding dense aggregated point clouds (right)
  </p>
 </div>
 <div class="wp-caption alignleft" id="attachment_22594" style="width: 193px">
  <a href="http://blog.kitware.com/wp-content/uploads/2018/05/DrillerFoundParaviewResized.png">
   <img alt="" class="wp-image-22594" height="216" sizes="(max-width: 183px) 100vw, 183px" src="http://blog.kitware.com/wp-content/uploads/2018/05/DrillerFoundParaviewResized.png" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/DrillerFoundParaviewResized.png 415w, https://blog.kitware.com/wp-content/uploads/2018/05/DrillerFoundParaviewResized-254x300.png 254w, https://blog.kitware.com/wp-content/uploads/2018/05/DrillerFoundParaviewResized-220x260.png 220w, https://blog.kitware.com/wp-content/uploads/2018/05/DrillerFoundParaviewResized-250x295.png 250w, https://blog.kitware.com/wp-content/uploads/2018/05/DrillerFoundParaviewResized-355x419.png 355w, https://blog.kitware.com/wp-content/uploads/2018/05/DrillerFoundParaviewResized-90x106.png 90w" width="183"/>
  </a>
  <p class="wp-caption-text">
   Fig. 5: Localization of the driller. Red points fit the model
  </p>
 </div>
 <p>
 </p>
 <p>
  Once the whole scene is fully scanned (in 20 seconds), a pattern recognition algorithm using drillers prior models is launched to detect and locate the driller.
 </p>
 <p>
 </p>
 <p>
  The driller location parameters are then transmitted to the robot. With the driller itself being located, the system can move on to the second phase: decide how to stack the tubes. For each new tube, the robot will launch a “tube scan” phase to acquire a high density point cloud around the location where the previous tube should have been placed.
 </p>
 <div class="wp-caption aligncenter" id="attachment_22606" style="width: 1034px">
  <a href="http://blog.kitware.com/wp-content/uploads/2018/05/Tube_scannedParaview-1.png">
   <img alt="" class="wp-image-22606 size-large" height="412" sizes="(max-width: 1024px) 100vw, 1024px" src="http://blog.kitware.com/wp-content/uploads/2018/05/Tube_scannedParaview-1-1024x412.png" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/Tube_scannedParaview-1-1024x412.png 1024w, https://blog.kitware.com/wp-content/uploads/2018/05/Tube_scannedParaview-1-300x121.png 300w, https://blog.kitware.com/wp-content/uploads/2018/05/Tube_scannedParaview-1-768x309.png 768w, https://blog.kitware.com/wp-content/uploads/2018/05/Tube_scannedParaview-1-220x88.png 220w, https://blog.kitware.com/wp-content/uploads/2018/05/Tube_scannedParaview-1-250x101.png 250w, https://blog.kitware.com/wp-content/uploads/2018/05/Tube_scannedParaview-1-355x143.png 355w, https://blog.kitware.com/wp-content/uploads/2018/05/Tube_scannedParaview-1-730x294.png 730w, https://blog.kitware.com/wp-content/uploads/2018/05/Tube_scannedParaview-1-90x36.png 90w" width="1024"/>
  </a>
  <p class="wp-caption-text">
   Fig. 6: Robot in “tube scan” position (left) and its corresponding point cloud (right)
  </p>
 </div>
 <p>
 </p>
 <div class="wp-caption alignright" id="attachment_22551" style="width: 236px">
  <a href="http://blog.kitware.com/wp-content/uploads/2018/05/TubeInlier.png">
   <img alt="" class="wp-image-22551 " height="208" sizes="(max-width: 226px) 100vw, 226px" src="http://blog.kitware.com/wp-content/uploads/2018/05/TubeInlier-300x276.png" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/TubeInlier-300x276.png 300w, https://blog.kitware.com/wp-content/uploads/2018/05/TubeInlier-768x707.png 768w, https://blog.kitware.com/wp-content/uploads/2018/05/TubeInlier-220x203.png 220w, https://blog.kitware.com/wp-content/uploads/2018/05/TubeInlier-250x230.png 250w, https://blog.kitware.com/wp-content/uploads/2018/05/TubeInlier-355x327.png 355w, https://blog.kitware.com/wp-content/uploads/2018/05/TubeInlier-730x672.png 730w, https://blog.kitware.com/wp-content/uploads/2018/05/TubeInlier-90x83.png 90w, https://blog.kitware.com/wp-content/uploads/2018/05/TubeInlier.png 792w" width="226"/>
  </a>
  <p class="wp-caption-text">
   Fig. 7: Localization of the tube. The reds points are those detected as belonging to the tube
  </p>
 </div>
 <p>
  Then, within this point cloud, a second pattern recognition algorithm is launched, looking for a cylinder model to detect and locate the previous tube. The recovered position, angle, length, radius and axis of the tube are transmitted to the robot.
  <br/>
  The robot will then place the next tube in position according to the position and orientation transmitted by the vision algorithms. The recovered position accuracy is below half a centimeter.
 </p>
 <p>
 </p>
 <div class="wp-caption alignleft" id="attachment_22586" style="width: 335px">
  <a href="http://blog.kitware.com/wp-content/uploads/2018/05/TubePlaced.png">
   <img alt="" class="wp-image-22586" height="252" sizes="(max-width: 325px) 100vw, 325px" src="http://blog.kitware.com/wp-content/uploads/2018/05/TubePlaced-1024x795.png" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/TubePlaced-1024x795.png 1024w, https://blog.kitware.com/wp-content/uploads/2018/05/TubePlaced-300x233.png 300w, https://blog.kitware.com/wp-content/uploads/2018/05/TubePlaced-768x596.png 768w, https://blog.kitware.com/wp-content/uploads/2018/05/TubePlaced-220x171.png 220w, https://blog.kitware.com/wp-content/uploads/2018/05/TubePlaced-250x194.png 250w, https://blog.kitware.com/wp-content/uploads/2018/05/TubePlaced-355x276.png 355w, https://blog.kitware.com/wp-content/uploads/2018/05/TubePlaced-730x567.png 730w, https://blog.kitware.com/wp-content/uploads/2018/05/TubePlaced-90x70.png 90w, https://blog.kitware.com/wp-content/uploads/2018/05/TubePlaced.png 1367w" width="325"/>
  </a>
  <p class="wp-caption-text">
   Fig. 8: New tube being automatically placed according to the position transmitted by vision algorithm
  </p>
 </div>
 <p>
 </p>
 <p>
 </p>
 <p>
 </p>
 <p>
  <em>
   Et voilà
  </em>
  , we made the robot arm do it!
  <br/>
  It takes now 30 seconds to add a new tube, versus 5 minutes with the previous manual method.
 </p>
 <p>
 </p>
 <p>
 </p>
 <p>
 </p>
 <p>
 </p>
 <p>
 </p>
 <p style="text-align: center">
  <b>
   This work was globally funded by Actemium Poissy.
  </b>
 </p>
 <p>
  <img alt="" class="size-medium wp-image-22544 aligncenter" height="100" sizes="(max-width: 300px) 100vw, 300px" src="http://blog.kitware.com/wp-content/uploads/2018/05/actemium_logo-300x100.jpg" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/actemium_logo-300x100.jpg 300w, https://blog.kitware.com/wp-content/uploads/2018/05/actemium_logo-220x74.jpg 220w, https://blog.kitware.com/wp-content/uploads/2018/05/actemium_logo-250x84.jpg 250w, https://blog.kitware.com/wp-content/uploads/2018/05/actemium_logo-355x119.jpg 355w, https://blog.kitware.com/wp-content/uploads/2018/05/actemium_logo-90x30.jpg 90w, https://blog.kitware.com/wp-content/uploads/2018/05/actemium_logo.jpg 517w" width="300"/>
 </p>
 <p style="text-align: center">
  <b>
   Software development was done by Kitware SAS.
  </b>
 </p>
 <p>
  <img alt="" class="size-medium wp-image-22545 aligncenter" height="78" sizes="(max-width: 300px) 100vw, 300px" src="http://blog.kitware.com/wp-content/uploads/2018/05/kitware_logo-300x78.png" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/kitware_logo-300x78.png 300w, https://blog.kitware.com/wp-content/uploads/2018/05/kitware_logo-220x57.png 220w, https://blog.kitware.com/wp-content/uploads/2018/05/kitware_logo-250x65.png 250w, https://blog.kitware.com/wp-content/uploads/2018/05/kitware_logo-355x92.png 355w, https://blog.kitware.com/wp-content/uploads/2018/05/kitware_logo-90x23.png 90w, https://blog.kitware.com/wp-content/uploads/2018/05/kitware_logo.png 441w" width="300"/>
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="size-full wp-image-22529 alignleft" height="150" sizes="(max-width: 125px) 100vw, 125px" src="http://blog.kitware.com/wp-content/uploads/2018/05/bjacquet.png" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/bjacquet.png 125w, https://blog.kitware.com/wp-content/uploads/2018/05/bjacquet-90x108.png 90w" width="125"/>
  <strong>
   Bastien Jacquet, PhD
  </strong>
  is a Technical Leader at Kitware, France.
  <br/>
  Since 2016, he promotes and extends Kitware’s Computer Vision expertise in Europe. He has experience in various Computer Vision and Machine Learning subdomains, and he is an expert in 3D Reconstruction, SLAM, and related mathematical tools.
 </p>
 <p>
 </p>
 <p>
  <img alt="" class="size-full wp-image-22530 alignleft" height="150" sizes="(max-width: 125px) 100vw, 125px" src="http://blog.kitware.com/wp-content/uploads/2018/05/Pierre.png" srcset="https://blog.kitware.com/wp-content/uploads/2018/05/Pierre.png 125w, https://blog.kitware.com/wp-content/uploads/2018/05/Pierre-90x108.png 90w" width="125"/>
  <strong>
   Pierre Guilbert
  </strong>
  is a computer vision R&amp;D engineer at Kitware, France.
  <br/>
  He has experience in Computer Vision, Machine Learning Classification for medical imagery and image registration in 2D and 3D. He is familiar with 2D and 3D processing techniques and related mathematical tools
  <br/>
  Since 2016, he works at Kitware on multiple projects related to multiple-view geometry, SLAM and point-cloud reconstruction and analysis.
 </p>
</div>
