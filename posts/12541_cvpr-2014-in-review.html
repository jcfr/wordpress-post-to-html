<meta charset="utf-8">
<div class="entry-content">
 <p dir="ltr" id="docs-internal-guid-d3635b7d-25d8-ccf0-9372-ed90c7ba0dd4">
  Eran Swears, Anthony Hoogs, Matt Leotta, and Sangmin Oh attended the IEEE Conference on Computer Vision and Pattern Recognition, which took place from June 23 to June 28, 2014 in Columbus, OH. CVPR is the premier annual conference for computer vision research, with more than 2000 attendees and a paper acceptance rate below 30%. Kitware participated in the main conference as well as several co-located workshops, presentations, and short courses including:
 </p>
 <ul style="list-style-type: disc;">
  <li dir="ltr">
   <p dir="ltr">
    A paper in the main conference, on “Complex Activity Recognition using Granger Constrained DBN in Sports and Surveillance Video”
   </p>
  </li>
  <li dir="ltr">
   <p dir="ltr">
    An invited talk on “Video Scene Segmentation and Recognition by Location-Independent Activity Classes” at the Workshop on Perceptual Organization in Computer Vision
   </p>
  </li>
  <li dir="ltr">
   <p dir="ltr">
    A demonstration of Complex Activity Recognition and Functional Scene Element Recognition
   </p>
  </li>
  <li dir="ltr">
   <p dir="ltr">
    A poster presentation on Collaborative Computer Vision Research in the Vision Entrepreneurs Workshop
   </p>
  </li>
  <li dir="ltr">
   <p dir="ltr">
    A half day tutorial on Emerging Topics in Human Activity Recognition
   </p>
  </li>
  <li dir="ltr">
   <p dir="ltr">
    A poster presentation on “Pyramid Coding for Functional Scene Element Recognition in Video Scenes” in the Scene Understanding Workshop
   </p>
  </li>
  <li dir="ltr">
   <p dir="ltr">
    A thesis research summary was presentation at the Doctoral Consortium
   </p>
  </li>
 </ul>
 <p dir="ltr">
  <strong>
   Main Conference
  </strong>
 </p>
 <p dir="ltr">
  At the main conference, Eran and Anthony presented a poster covering their CVPR paper “Complex Activity Recognition using Granger Constrained DBN in Sports and Surveillance Video,” which was also featured as a
  <a href="http://techtalks.tv/talks/complex-activity-recognition-using-granger-constrained-dbn-gcdbn-in-sports-and-surveillance-video/59821/">
   video spotlight
  </a>
  . The poster presentation was a success, as the crowd around the poster remained consistent throughout the two-hour presentation session.
 </p>
 <p dir="ltr">
  The conference also included video spotlights of the posters, which played before and during the poster sessions. These videos are available
  <a href="http://techtalks.tv/events/329/844/">
   online
  </a>
  . The poster spotlights were effective in helping people determine what posters they wanted to visit before the poster sessions began.
 </p>
 <p dir="ltr">
  <strong>
   Activity Detection and Scene Understanding Demos
  </strong>
 </p>
 <p>
  Eran, Matt, Anthony, and Sangmin gave a live demonstration in the Activity Detection and Scene Understanding demo session. The demo showed a user defining a complex activity as a graphical model using the tools that Rusty and the team developed in vpView; automatic, live detection of the activity on a video dataset; and user examination of the results in vpView. The team displayed the actual graphical user interface (GUI) and how the user interacted with it, while running detection algorithms a couple of times!
 </p>
 <p>
  The demo was reasonably well-attended and worthwhile. There was an ebb and flow of the crowd around the demo due to a parallel poster session. The combination of PowerPoint presentations, demo software, and multiple computers for simultaneous display was very beneficial to clearly represent this capability.
 </p>
 <p>
  <a href="/blog/files/331_233986456.jpg" rel="noopener" target="_blank">
   <img src="https://blog.kitware.com/blog/files/Small.331_233986456.jpg" width="100%"/>
  </a>
 </p>
 <p>
  <a href="/blog/files/331_1678325092.jpg" rel="noopener" target="_blank">
   <img src="https://blog.kitware.com/blog/files/Small.331_1678325092.jpg" width="100%"/>
  </a>
 </p>
 <p dir="ltr" id="docs-internal-guid-d3635b7d-25da-03b1-7af6-37bfa205c2e3">
  <strong>
   Vision Industry &amp; Entrepreneur Workshop (VIEW)
  </strong>
 </p>
 <p dir="ltr">
  This workshop included several invited talks and posters about research and entrepreneurship in the computer vision industry. Kitware displayed a poster that provided an overview of its computer vision work. The poster, created by Arslan Basharat and Brad Davis, was presented by Matt, Anthony, and Sangmin. It sparked interesting discussions about what it means to participate in collaborative research in the industry.
 </p>
 <p dir="ltr">
  <strong>
   Emerging Topics in Human Activity Recognition Tutorial
  </strong>
 </p>
 <p dir="ltr">
  Sangmin was a co-organizer of this tutorial with Michael Ryoo (JPL), Ivan Laptev (INRIA), and Greg Mori (Simon Fraser University). The tutorial covered a wide variety of topics, including visual features, group activity recognition, first person videos (i.e., those taken using Google Glass), and applications of activity recognition. The tutorial was a success with around 50 attendees. Sangmin showcased Kitware’s work on complex activity recognition, WAMI tracking, and sports activity analysis. The slides for this tutorial can be found
  <a href="http://cvrc.ece.utexas.edu/mryoo/cvpr2014tutorial/">
   online
  </a>
  .
 </p>
 <p dir="ltr">
  <strong>
   Workshop on Perceptual Organization in Computer Vision
  </strong>
 </p>
 <p dir="ltr">
  This workshop was very worthwhile, with a strong program of invited speakers including Anthony, who gave a talk titled “Video Scene Segmentation and Recognition by Location-Independent Activity Classes.” The talk discussed action detection and matching in VIRAT, with an emphasis on finding short-duration events in long, continuous video archives. (Most action recognition datasets work with short clips containing single actions.)
 </p>
 <p dir="ltr">
  <strong>
   Scene Understanding Workshop
  </strong>
 </p>
 <p dir="ltr">
  This workshop was enlightening and informative, consisting of invited talks and a poster session, during which several presenters discussed trends in predicting information in a scene. Such trends include predicting information outside of an image’s bounds and creating a list of possible futures for a vehicle or person’s trajectory. In addition, a lot of work was discussed in scene understanding, particularly from single images. However, work in this area has started to move into video over the past several years. Many of the ideas and observations from single image analysis can be applied to video. The workshop also highlighted using 3D modeling to improve segmentation and recognition of objects. In particular, there were typically very large improvements in performance when incorporating 3D CAD models of objects.
 </p>
 <p dir="ltr">
  Additionally, Eran presented a 30 second poster spotlight and spent a couple hours presenting Kitware’s previous ICCV paper, “
  <a href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Swears_Pyramid_Coding_for_2013_ICCV_paper.pdf">
   Pyramid Coding for Functional Scene Element Recognition in Video Scenes
  </a>
  .” In general, the other scene understanding posters at the workshop were applied to high resolution images with well defined objects and used pixel features. As a result, there was not as much interest in our more challenging problem as we would have liked, given that our applications have many, sometimes ill defined, objects in low resolution imagery, and scene understanding is performed using motion behaviors rather than pixel features. In any case, Eran did meet a few people that were interested in doing internships and possibly full-time work at Kitware.
 </p>
 <p dir="ltr">
  <strong>
   Conference Papers
  </strong>
 </p>
 <p dir="ltr">
  This year, there were 1807 total
  <a href="http://www.pamitc.org/cvpr14/program.php">
   paper
  </a>
  submissions, 540 of which were accepted for the conference and 104 of which were presented at one of the oral presentation sessions. This makes the acceptance rate of papers for the conference 29.88 percent. The percent of papers selected for oral presentation was 5.76. The papers can be downloaded
  <a href="http://www.cvpapers.com/cvpr2014.html">
   online
  </a>
  .
 </p>
 <p dir="ltr">
  It was very satisfying to see that many of the CVPR 2014 papers used the VIRAT public dataset (from Kitware’s CVPR 2011 paper, “A Large-scale Benchmark Dataset for Event Recognition in Surveillance Video”) for activity prediction and classification. Kitware’s team compiled the dataset with university collaborators and made it available to the wider computer vision community for research. It is one of the largest collections of surveillance footage to be made public.
 </p>
 <p dir="ltr">
  So far, the CVPR 2011 paper has been cited over 90 times in conference proceedings and journals, and this number is growing!
 </p>
 <p dir="ltr">
  <strong>
   Visual SLAM &amp; State of the Art 3D Reconstruction Techniques Tutorials
  </strong>
 </p>
 <p dir="ltr">
  Matt attended two closely-related tutorials on Visual SLAM (Simultaneous Localization and Mapping) and Structure from Motion (SfM). SLAM comes from the robotics community and has a focus on real-time performance on video. SfM, on the other hand, often operates in batch on collections of unordered images. Both tutorials gave good overviews of key contributions to the field. Both discussed sparse 3D reconstruction (point clouds) and dense 3D reconstruction (surfaces). The lines between SfM and SLAM are becoming increasingly blurred. Kitware’s recently released
  <a href="https://github.com/kitware/maptk">
   MAP-Tk
  </a>
  is intended to eventually span both of these topics.
 </p>
 <p dir="ltr">
  <strong>
   Concluding Thoughts
  </strong>
 </p>
 <p dir="ltr">
  In general, the conference was very productive and comparable to ICCV. There was a lot of interest around Kitware’s work, as well as potential job and internship opportunities. As expected from a top tier conference, the team found that the state of the art is continuing to make significant progress in  tracking, scene understanding, 3D reconstruction, and deep learning to name a few. Of particular note, the state of the art in tracking, even on WAMI data, is being pushed to new limits providing us with some useful tools to integrate into our tracker!
 </p>
 <p>
 </p>
</div>
