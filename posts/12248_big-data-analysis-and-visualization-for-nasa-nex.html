<meta charset="utf-8">
<div class="entry-content">
 <p>
  <strong style="line-height: 1.6em;">
   Authors
  </strong>
  <span style="line-height: 1.6em;">
   –
  </span>
  Aashish
  <span style="line-height: 1.6em;">
  </span>
  Chaudhary
  <span style="line-height: 1.6em;">
   (PI),
  </span>
  Petr
  <span style="line-height: 1.6em;">
  </span>
  Votava
  <span style="line-height: 1.6em;">
   (NASA, CO-I), Chris
  </span>
  Kotfila
  <span style="line-height: 1.6em;">
   , Michael
  </span>
  Grauer
  <span style="line-height: 1.6em;">
   , Jonathan
  </span>
  Beezley
  <span style="line-height: 1.6em;">
   , Andrew
  </span>
  Michaelis
  <span style="line-height: 1.6em;">
   (NASA), Dr. Rama
  </span>
  Nemani
  <span style="line-height: 1.6em;">
   (NASA, Chief Scientist)
  </span>
 </p>
 <p>
  <span style="line-height: 1.6em;">
   <strong>
    Background
   </strong>
   –
  </span>
  NEX
  <span style="line-height: 1.6em;">
   is a collaborative platform that brings together a state-of-the-art computing facility with large volumes (hundreds of terabytes) of NASA satellite and climate data as well as number of modeling and data analysis tools and services. In order to facilitate a broader community engagement,
  </span>
  NEX
  <span style="line-height: 1.6em;">
   has deployed a cloud component –
  </span>
  OpenNEX
  <span style="line-height: 1.6em;">
   , which provides access to a number of NASA datasets together with tools and services, hands-on tutorials and documentation.
  </span>
 </p>
 <p>
  <span style="line-height: 1.6em;">
   <strong>
    Technical Objective
   </strong>
   –
  </span>
  <span style="line-height: 1.6em;">
   The goal of this project is to develop capabilities for an integrated collaborative
  </span>
  petabyte-scale
  <span style="line-height: 1.6em;">
   Earth science data analysis and visualization environment. We will deploy this environment within the NASA Earth Exchange (
  </span>
  NEX
  <span style="line-height: 1.6em;">
   ) in order to enhance existing science data analysis capabilities in both high-performance computing (
  </span>
  HPC
  <span style="line-height: 1.6em;">
   ) and cloud environments. This system will significantly enhance the ability of the scientific community to accelerate transformation of Earth science observational data from NASA missions, model outputs and other sources into science information and knowledge. We propose to develop a web-based system that seamlessly interfaces with both
  </span>
  HPC
  <span style="line-height: 1.6em;">
   and cloud environments, providing tools that enable science teams to develop and deploy large-scale data analysis and visualization pipelines and enable sharing results with the community. The
  </span>
  HPC
  <span style="line-height: 1.6em;">
   component will interface with the NASA Earth Exchange (
  </span>
  NEX
  <span style="line-height: 1.6em;">
   ), a collaboration platform for the Earth science community that provides a mechanism for scientific collaboration, knowledge and data sharing together with direct access to over
  </span>
  1PB
  <span style="line-height: 1.6em;">
   of Earth science data and 200,000-cores processing system.
  </span>
 </p>
 <p>
  <a href="/blog/files/36_513999244.png" target="_blank">
   <img src="https://blog.kitware.com/blog/files/Small.36_513999244.png" width="100%"/>
  </a>
 </p>
 <p>
  Figure 1: Minerva screenshort showing NASA's global precipitation dataset and other layers in GeoJS
 </p>
 <p>
  <strong>
   <span style="line-height: 1.6em;">
    Architecture –
   </span>
  </strong>
  <span style="line-height: 1.6em;">
   NASA already maintains a wide array of data on Amazon’s
  </span>
  S3
  <span style="line-height: 1.6em;">
   service (
  </span>
  <a href="https://aws.amazon.com/nasa/nex/" style="line-height: 1.6em;">
   https://aws.amazon.com/nasa/nex/
  </a>
  <span style="line-height: 1.6em;">
   ). This makes the
  </span>
  AWS
  <span style="line-height: 1.6em;">
  </span>
  EC2
  <span style="line-height: 1.6em;">
   cloud infrastructure a natural choice for deploying custom clusters for batch and interactive data processing. Our approach provides a custom web-based interface leveraging a number of technologies in the [Resonant] (
  </span>
  <a href="http://resonant.kitware.com/" style="line-height: 1.6em;">
   http://resonant.kitware.com/
  </a>
  <span style="line-height: 1.6em;">
   ) stack. These include
  </span>
  <a href="https://girder.readthedocs.org/en/latest/" style="line-height: 1.6em;">
   Girder
  </a>
  <span style="line-height: 1.6em;">
   for data management,
  </span>
  <a href="http://romanesco.readthedocs.org/en/latest/" style="line-height: 1.6em;">
   Romanesco
  </a>
  <span style="line-height: 1.6em;">
   for
  </span>
  asychronous
  <span style="line-height: 1.6em;">
   workflow execution and most
  </span>
  importaintly
  <span style="line-height: 1.6em;">
  </span>
  <a href="http://minervadocs.readthedocs.org/en/latest/" style="line-height: 1.6em;">
   Minerva
  </a>
  <span style="line-height: 1.6em;">
   and
  </span>
  <a href="http://geojs.readthedocs.org/en/latest/" style="line-height: 1.6em;">
   GeoJS
  </a>
  <span style="line-height: 1.6em;">
   for visualization. This stack is deployed to an ‘always on’ cloud server that provides user authentication and persistent session based access to the geospatial analytic and visualization tools.
  </span>
 </p>
 <p>
  <a href="/blog/files/36_819635634.jpg" target="_blank">
   <img src="https://blog.kitware.com/blog/files/Small.36_819635634.jpg" width="100%"/>
  </a>
  Figure 2: Minerva / Kitware-NEX open source tools stack and architecture.
 </p>
 <p>
 </p>
 <p>
  When large scale distributed data analysis pipelines are required, Minerva will provide management tools for interfacing with existing HPC resources as well as launching “On Demand” scalable preconfigured clusters that leverage Hadoop HDFS and Spark for distributed short-term storage and computing.
 </p>
 <p>
  In the next few blogs, we will describe more of our work on Minerva application and big data use-cases that we are solving that involves running analysis on terabytes of datasets on cloud instances.
 </p>
 <p>
  <strong>
   Acknowledgement
  </strong>
  –
  <strong>
  </strong>
  <span style="line-height: 1.6em;">
   We are thankful to NASA’s Science Mission Directorate, NASA Headquarters for providing this opportunity.
  </span>
 </p>
</div>
