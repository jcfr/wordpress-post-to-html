<meta charset="utf-8">
<div class="entry-content">
 <p>
  The Journal
  <strong>
   Nature
  </strong>
  just published a very interesting article:
  <br/>
  <a href=" http://www.nature.com/news/2010/101013/full/467775a.html">
   http://www.nature.com/news/2010/101013/full/467775a.html
  </a>
 </p>
 <p>
  <strong>
   “Computational science: …Error”
  </strong>
  <br/>
  <strong>
   “…why scientific programming does not compute.”
  </strong>
  <br/>
  by Zeeya Merali
 </p>
 <p>
  In this article, Zeeya Merali discusses how scientific research has come to strongly depend on software development, but may not have caught up with the good practices of software engineering that are required for developing high quality software.
 </p>
 <p>
  <strong>
   Some excerpts:
  </strong>
 </p>
 <ul>
  <li>
   <em>
    “A quarter of a century ago, most of the computing work done by scientists was relatively straightforward. But as computers and programming tools have grown more complex, scientists have hit a “steep learning curve”, says James Hack, director of the US National Center for Computational Sciences at Oak Ridge National Laboratory in Tennessee. “The level of effort and skills needed to keep up aren’t in the wheelhouse of the average scientist.”
   </em>
  </li>
 </ul>
 <p>
  Unfortunately, essential practices of software engineering are not taught or practiced enough by scientists:
 </p>
 <ul>
  <li>
   <em>
    “As a general rule, researchers do not test or document their programs rigorously, and they rarely release their codes, making it almost impossible to reproduce and verify published results generated by scientific software, say computer scientists. At best, poorly written programs cause researchers such as Harry to waste valuable time and energy. But the coding problems can sometimes cause substantial harm, and have forced some scientists to retract papers.”
   </em>
  </li>
 </ul>
 <p>
  and clearly identify that
  <strong>
   Openness
  </strong>
  is an important element for fixing this situation
 </p>
 <ul>
  <li>
   <em>
    “As recognition of these issues has grown, software experts and scientists have started exploring ways to improve the codes used in science. Some efforts teach researchers important programming skills, whereas others encourage collaboration between scientists and software engineers, and teach researchers to be more open about their code. “
   </em>
  </li>
 </ul>
 <p>
  The lack of training in software engineering becomes evident once you start looking into the matter:
 </p>
 <ul>
  <li>
   <em>
    “In 2008, he and his colleagues conducted an online survey of almost 2,000 researchers, from students to senior academics, who were working with computers in a range of sciences. What he found was worse than he had anticipated1 (see ‘Scientists and their software’). “There are terrifying statistics showing that almost all of what scientists know about coding is self-taught,” says Wilson. “They just don’t know how bad they are.”
   </em>
  </li>
 </ul>
 <p>
  There are many documented cases on how low quality software leads researchers and their communities to errors, and make them lose time, resources, and credibility:
 </p>
 <ul>
  <li>
   <em>
    “As a result, codes may be riddled with tiny errors that do not cause the program to break down, but may drastically change the scientific results that it spits out. One such error tripped up a structural-biology group led by Geoffrey Chang of the Scripps Research Institute in La Jolla, California. In 2006, the team realized that a computer program supplied by another lab had flipped a minus sign, which in turn reversed two columns of input data, causing protein crystal structures that the group had derived to be inverted. Chang says that the other lab provided the code with the best intentions, and “you just trust the code to do the right job”.
    <strong>
     His group was forced to retract five papers
    </strong>
    published in Science, the Journal of Molecular Biology  and Proceedings of the National Academy of Sciences,
    <strong>
     and now triple checks everything
    </strong>
    , he says.”
   </em>
  </li>
 </ul>
 <p>
  The adoption of
 </p>
 <ul>
  <li>
   Openness
  </li>
  <li>
   Collaboration Platforms
  </li>
  <li>
   Reproducibility in scientific publications
  </li>
 </ul>
 <p>
  are also identified as important elements that can help correct this dramatic situation.
 </p>
 <p>
  It is well known that closed source code, home-grown in labs over generations of graduate students becomes unmaintainable:
 </p>
 <ul>
  <li>
   <em>
    “Problems created by bad documentation are further amplified when  successful codes are modified by others to fit new purposes. The result  is the bane of many a graduate student or postdoc’s life: the ‘monster  code’. Sometimes decades old, these codes are notoriously messy and  become progressively more nightmarish to handle, say computer  scientists.”
   </em>
  </li>
 </ul>
 <ul>
  <li>
   <em>
    “The mangled coding of these monsters can sometimes make it difficult to check for errors. One example is a piece of code written to analyse the products of high-energy collisions at the Large Hadron Collider particle accelerator at CERN, Europe’s particle-physics laboratory near Geneva, Switzerland. The code had been developed over more than a decade by 600 people, “some of whom are excellent programmers and others who do not really know how to code very well”, says David Rousseau, software coordinator for the ATLAS experiment at CERN. Wilson and his students tried to test the program, but they could not get very far: the code would not even run on their machines.”
   </em>
  </li>
 </ul>
 <p>
  <strong>
   Open Source Software to the Rescue:
  </strong>
  <br/>
  By opening and sharing the code, communities can distribute the burden of maintenance cost, and improve their quality control practices by having many eyes looking for defects in the code. The
  <strong>
   Visualization Toolkit (VTK)
  </strong>
  is presented as a successful example on how to manage complexity in scientific software:
 </p>
 <ul>
  <li>
   <em>
    Some software developers have found ways to combat the growth of monster  code. One example is the
    <a href="http://www.vtk.org/">
     Visualization Toolkit,
    </a>
    an
    <a href="http://www.vtk.org/VTK/project/license.html">
     open-source
    </a>
    , freely  available software system for three-dimensional computer graphics.  People can modify the software as they wish, and it is
    <a href="http://www.cdash.org/CDash/index.php?project=VTK">
     rerun each night  on every computing platform
    </a>
    that supports it, with the
    <a href="http://www.cdash.org/CDash/index.php?project=VTK">
     results published  on the web
    </a>
    . The process ensures that the software will work the same  way on different systems.
   </em>
  </li>
 </ul>
 <p>
  and Merali, immediately put the finger in the problem:
  <em>
   <br/>
  </em>
 </p>
 <ul>
  <li>
   <em>
    That kind of openness has yet to infiltrate the scientific research world, where many leading science journals, including  Nature,  Science  and  Proceedings of the National Academy of Sciences,
    <strong>
     do not insist that authors make their code available.
    </strong>
    Rather, they  require that authors
    <strong>
     provide enough information for results to be  reproduced
    </strong>
    .
   </em>
  </li>
 </ul>
 <p>
  But in today’s software-driven research environment, the truth is that
 </p>
 <ul>
  <li>
   It is impossible to reproduce research without having access to the software used to run the experiments, and
  </li>
  <li>
   It is impossible to fit, in the pages of a standard paper, the instructions required to re-implement any piece of software (assuming that another group was willing to spend months reimplementing that software).
  </li>
 </ul>
 <p>
  Therefore, the papers that describe work relying on computational methods can only give the
  <strong>
   appearance
  </strong>
  of reproducibility; however, they fail to deliver on that promise to the readers.
 </p>
 <p>
  The fact that scientific journals and conference publications have abandoned the healthy practice of requiring
  <strong>
   REPRODUCIBILITY
  </strong>
  , which used to be the
  <a href="http://reproducibleresearch.net/index.php/Main_Page">
   hallmark of scientific work
  </a>
  , has lead us to this quagmire. In today’s scientific work, in order to enable reproducibility by external groups, the following are indispensable elements:
 </p>
 <ul>
  <li>
   Open Source Software
  </li>
  <li>
   Open Data
  </li>
  <li>
   Full disclosure of parameters
  </li>
 </ul>
 <p>
  The Scientific paper must to abandon it current “marketing” speech, and return to its origins, rooted in the principle of
  <strong>
  </strong>
 </p>
 <p>
  Enabling Independent Groups to replicate the Work
 </p>
</div>